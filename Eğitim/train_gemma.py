import torch
import os
import sys
import gc
from packaging import version
import transformers
from transformers import AutoModelForCausalLM, GemmaTokenizerFast, BitsAndBytesConfig
from trl import SFTTrainer, SFTConfig
from peft import LoraConfig
from datasets import load_dataset

# === BELLEK PAR√áALANMASINI √ñNLEME ===
os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"

# === TEMƒ∞ZLƒ∞K ===
gc.collect()
torch.cuda.empty_cache()

print(f"üîç Transformers S√ºr√ºm√º: {transformers.__version__}")

# === MODEL AYARI ===
local_model_path = "/home/ozhan/gemma-2-2b-it" 
hub_model_id = "google/gemma-2-2b-it"

if not os.path.exists(local_model_path):
    print(f"‚ùå Model klas√∂r√º bulunamadƒ±: {local_model_path}")
    exit()

print(f"üîÑ Model y√ºkleniyor: {local_model_path}...")

# === 4-bit QLoRA Config ===
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
)

# === Tokenizer ===
try:
    print("üî§ Tokenizer internetten y√ºkleniyor...")
    tokenizer = GemmaTokenizerFast.from_pretrained(hub_model_id)
    tokenizer.padding_side = "right"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
except Exception as e:
    print(f"‚ùå Tokenizer hatasƒ±: {e}")
    exit()

# === Dataset ===
if not os.path.exists("dataset.jsonl"):
    print("‚ùå 'dataset.jsonl' yok!")
    exit()

dataset = load_dataset("json", data_files="dataset.jsonl")["train"]

def format_chat_template(example):
    return f"<start_of_turn>user\n{example['instruction']}\n\nVERƒ∞:\n{example['input']}<end_of_turn>\n<start_of_turn>model\n{example['output']}<end_of_turn>"

dataset = dataset.map(lambda x: {"text": format_chat_template(x)}, batched=False)

# === LoRA Config (Mƒ∞Nƒ∞MUM AYARLAR) ===
lora_config = LoraConfig(
    r=4,
    lora_alpha=16,
    lora_dropout=0.05,
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj"] 
)

# === Model Y√ºkleme ===
print("üß† Model VRAM'e y√ºkleniyor...")
model = AutoModelForCausalLM.from_pretrained(
    local_model_path,
    quantization_config=bnb_config,
    device_map="auto",
    attn_implementation="eager" 
)
model.config.use_cache = False

# === SFTConfig (ULTRA HAFƒ∞F) ===
training_args = SFTConfig(
    output_dir="ozhan-gemma-2b-lora",
    dataset_text_field="text",
    max_length=128,             # <-- 256'dan 128'e d√º≈üt√º (B√ºy√ºk tasarruf)
    packing=False,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={'use_reentrant': True}, # Bellek tasarrufu i√ßin True deniyoruz
    num_train_epochs=1,
    max_steps=50,
    learning_rate=2e-4,
    bf16=True,                      # <-- fp16 yerine bf16 (RTX 30 serisi i√ßin daha iyi)
    optim="paged_adamw_8bit",
    logging_steps=5,
    save_strategy="no",
    report_to="none"
)

# === Trainer ===
print("üöÄ Eƒüitim ba≈ülƒ±yor...")
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=lora_config,
    args=training_args,
    processing_class=tokenizer 
)

# Son temizlik
torch.cuda.empty_cache()

trainer.train()

print("üíæ Kaydediliyor...")
trainer.model.save_pretrained("ozhan-gemma-2b-lora")
tokenizer.save_pretrained("ozhan-gemma-2b-lora")
print("‚úî Bitti!")